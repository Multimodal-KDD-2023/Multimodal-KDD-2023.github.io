<!DOCTYPE html>
<html lang="en">
    <head>
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-S0FLVX7GFE"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'G-S0FLVX7GFE');
	</script>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <title>Multimodal KDD 2023: International Workshop on Multimodal Learning</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.css" rel="stylesheet"  type='text/css'>
        <link href="css/styles.css" rel="stylesheet" />
    </head>

    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
            <div class="container px-4">
                <a class="navbar-brand" href="#" style="font-size:x-large;">
                    <img src="assets/amz_logo.png" height="30" alt="">
                    <!-- &nbsp;&nbsp;<span><i class="fa-brands fa-amazon"></i></span> -->
                </a>    
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="nav navbar-nav nav-pills ms-auto" style="font-size:medium; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif">
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#home" >Home</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#keynotes" >Keynotes</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#papers" >Accepted Papers</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#organizers" >Organizer</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#schedule" >Other Information</a></li>&nbsp;&nbsp;
                </ul>
                </div>
            </div>
        </nav>
        <!-- Header-->
        <header class="text-white" style="background:transparent url('assets/opener.jpeg') no-repeat center center /cover; opacity: 0.7;">
            <div class="container px-4 text-center" 
            style="background: rgba(0, 0, 0, 0.8); font-size:medium; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif">
                <h1 class="fw-bolder" style="color:ivory;">Multimodal KDD 2023:</h1>
                <h2 class="fw-bolder">International Workshop on Multimodal Learning</h2>
                <p>Held in conjunction with <a style="color:#e77725;" href="https://kdd.org/kdd2023/">KDD'23</a></p>
            </div>
        </header>

        <!-- home section-->
        <section style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="home">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <br>
                        <h2 style="text-align: center;">Welcome to Multimodal KDD 2023 !</h2>
                        <p style="text-align: justify;">
                            The recent advancements in machine learning and artificial intelligence (particularly foundation models such as BERT, GPT-3, T5, ResNet, etc.) have demonstrated remarkable capabilities and driven significant revolutionary changes to the way we make inferences from complex data. These models represent a fundamental shift in the way data are approached and offer exciting new research directions and opportunities for multimodal learning and data fusion. <br> <br>
                            Given the potential of foundation models to transform the field of multimodal learning, there is a need to bring together experts and researchers to discuss the latest developments in this area, exchange ideas, and identify key research questions and challenges that need to be addressed. By hosting this workshop, we aim to create a forum for researchers to share their insights and expertise on multimodal data fusion and learning using foundation models, and to explore potential new research directions and applications in the rapidly evolving field. We expect contributions from interdisciplinary researchers to study and model interactions between (but not limited to) modalities of language, graphs, time-series, vision, tabular data, sensors, and more. Our workshop will emphasize interdisciplinary work and aim at seeding cross-team collaborations around new tasks, datasets, and models.
                        </p>
                        <strong>Contact:</strong> <a href="mailto:kdd2023-ws-multimodal@amazon.com">kdd2023-ws-multimodal@amazon.com</a> 
                    </div>
                </div>
            </div>
        </section>

        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="keynotes">
                <div class="row gx-4 justify-content-center">
                        <div class="col-lg-8">
                            <h2 style="text-align: center;">KEYNOTES</h2>
                            <h4 style="text-align: center;">Next-Generation Intelligent Assistants for AR/VR Devices
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="slides/keynote-1-slides.pdf" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                </div>  
                            </h4>

                        </div>
                </div>
                <br>
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-2">
                        <div>
                            <a class="navbar-brand js-scroll-trigger" href="https://lunadong.com/">
                            <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto" src="./assets/luna.jpeg" alt="
                                Xin Luna Dong"></span>
                            </a>
                        </div>
                        <br>
                        <div>
                            <a style="color:#0275d8; text-decoration: none; display: inline-block; text-align: justify;" target="_blank" href="https://lunadong.com/"><b>Xin (Luna) Dong </b><br> Head Scientist <br> Meta</a>
                        </div>
                    </div>
                    <div class="col-lg-6" style="text-align: justify;">
                        <b>Abstract:</b> An intelligent assistant shall be an agent that knows you and the world, can receive your requests or predict your needs, and provide you the right services at the right time with your permission. As smart devices such as Amazon Alexa, Google Home, Meta Ray-ban Stories get popular, Intelligent Assistants are gradually playing an important role in people's lives. The Emergence of AR/VR devices brings more opportunities and calls for the next generation of Intelligent Assistants. In this talk, we discuss the many challenges and opportunities we face to grow intelligent assistants from voice-only to multi-modal, from context-agnostic to context-aware, from listening to the users' requests to predicting the user's needs, and from server-side to on-device. We expect these new challenges to open doors to new research areas and start a new chapter for providing personal assistance services.
                        <br><br>
                        <b>Bio:</b> Xin Luna Dong is a Principal Scientist at Meta Reality Labs, working on Intelligent Assistant. She has spent more than a decade building knowledge graphs, such as Amazon Product Graph and Google Knowledge Graph. She has co-authored books "Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases" and “Big Data Integration”. She was awarded VLDB Women in Database Research Award (2023) for "significant contributions to knowledge graph construction and data integration", ACM Distinguished Member (2018) for "significant contributions to data and knowledge integration", and VLDB Early Career Research Contribution Award (2016) for “advancing the state of the art of knowledge fusion”. She serves in the VLDB endowment and PVLDB advisory committee, and is a PC co-chair for KDD'2022 ADS track, WSDM'2022, VLDB'2021, and Sigmod'2018.
                    </div>
                </div>
                
                <br><br>

                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h4 style="text-align: center;">Graph-based Fusion for Multimodal Learning</h4>
                    </div>
                </div>
                <br>
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-6" style="text-align: justify;">
                        <b>Abstract:</b> With the advances in data collection techniques, large amounts of multimodal data collected from multiple sources are becoming available. Such multimodal data can provide complementary information that can reveal fundamental characteristics of real-world subjects. Thus, multimodal machine learning has become an active research area. Extensive works have been developed to exploit multimodal interactions and integrate multi-source information. In this talk, we will discuss using a graph-based multimodal fusion approach to enable multimodal fusion of incomplete data within a heterogeneous graph structure. This approach develops a unique strategy for learning on incomplete multimodal data without data deletion or data imputation. Moreover, we will discuss a dynamic graph-based approach to support federated training over multimodal distributed data without assuming similar active sensors in all clients. The key idea is to employ a dynamic and multi-view graph structure to adaptively capture the correlations amongst multimodal client models.                    
                        <br><br>
                        <b>Short Bio:</b> Dr. Aidong Zhang is Thomas M. Linville Endowed Professor of Computer Science, with joint appointments at Data Science, and Biomedical Engineering at University of Virginia (UVA). Prof. Zhang’s research interests include machine learning, data science, bioinformatics, and health informatics. Prof. Zhang is a fellow of ACM, IEEE, and AIMBE.
                    </div>
                    <div class="col-lg-2">
                        <a class="navbar-brand js-scroll-trigger" href="https://www.cs.virginia.edu/~az9eg/website/home.html">
                        <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto" src="./assets/zhang.jpg" alt="Aidong Zhang"></span>
                        <p style="text-align: center;">
                            <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://www.cs.virginia.edu/~az9eg/website/home.html"><b>Aidong Zhang</b><br>Thomas M. Linville Professor of Computer Science<br>University of Virginia</a><br>
                        </p>                            
                        </a>
                    </div>
                </div>

            </div>

        </section>

        <section style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="papers">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2 style="text-align: center;">ACCEPTED PAPERS</h2>
                        <div class="list-group list-group-flush">
                            <a class="list-group-item list-group-item-action"></a>
                            <a class="list-group-item list-group-item-action">
                                <h5>Contrastive Multimodal Text Generation for E-Commerce Brand Advertising</h5>
                                <p>Nikhil Madaan, Krishna Kesari, Manisha Verma, Shaunak Mishra and Tor Steiner</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                <a href="papers/1-4 Contrastive Multimodal Text Generation for E-Commerce Brand Advertising.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception</h5>
                                <p>Daniel Shin, Gao Pei, Priyadarshini Kumari and Tarek Besold</p>
                                <!--div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="slides/1-2-slides.pdf" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="papers/1-2 Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div--> 
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>What Makes Good Open-Vocabulary Detector: A Disassembling Perspective</h5>
                                <p>Jincheng Li, Chunyu Xie, Xiaoyu Wu, Bin Wang and Dawei Leng</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="slides/1-5-slides.pdf" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="papers/1-5 What Makes Good Open-Vocabulary Detector.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                                
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>Explainable Local and Global Models for Fine-Grained Multimodal Product Recognition</h5>
                                <p>Tobias Pettersson, Maria Riveiro and Tuwe Löfström</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="slides/1-3-slides.pdf" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="papers/1-3 Explainable Local and Global Models for Fine-Grained Multimodal Product Recognition.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                             
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h5>
                                <p>Youxiang Zhu, Nana Lin, Xiaohui Liang, John Batsis, Robert Roth and Brian MacWhinney</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="slides/1-1-slides.pdf" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="papers/1-1 Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                                
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding</h5>
                                <p>Zheng Chen, Ziyan Jiang, Fan Yang, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu and Aram Galstyan</p>
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>Detecting text-rich objects: OCR or object detection? A case study with stopwatch detection</h5>
                                <p>Yarong Feng, Zongyi Liu, Yuan Ling, Shunyan Luo, Shujing Dong, Shuyi Wang and Bruce Ferry</p> 
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="papers/2-1 Detecting text-rich objects.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                            
                            </a>
                            
                            <a class="list-group-item list-group-item-action">
                                <h5>REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory</h5>
                                <p>Ziniu Hu</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="papers/2-2 REVEAL.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                               
                            </a>  
                            <a class="list-group-item list-group-item-action"></a>
                        </div>             
                    </div>
                </div>
            </div>
        </section>

        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4"  id="organizers">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <br>
                        <h2 style="text-align: center;">Organizers - Amazon Teams</h2>
                        <br><br>
                        <div class="row">
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://www.linkedin.com/in/yuan-ling-3572a75a/">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/ling.png" alt="Yuan Ling" ></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none;" target="_blank" href="https://www.linkedin.com/in/yuan-ling-3572a75a/"><b>Yuan Ling</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://wufanyou.github.io/">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/wu.png" alt="Fanyou Wu"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://wufanyou.github.io/"><b>Fanyou Wu</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://www.linkedin.com/in/shujing-dong">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/dong.jpeg" alt="Shujing Dong"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://www.linkedin.com/in/shujing-dong"><b>Shujing Dong</b></a><br>
                                </p>                            
                                </a>
                            </div>
                        </div>
                        <div class="row">
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://www.linkedin.com/in/yarong-feng-31bba172/">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/feng.png" alt="Yarong Feng"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://www.linkedin.com/in/yarong-feng-31bba172/"><b>Yarong Feng</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://cse.umn.edu/cs/george-karypis">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/karypis.jpeg" alt="George Karypis" ></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://cse.umn.edu/cs/george-karypis"><b>George Karypis</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://people.cs.vt.edu/~reddy/">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/reddy.png" alt="Chandan K Reddy"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://people.cs.vt.edu/~reddy/"><b>Chandan K Reddy</b></a><br>
                                </p>                            
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="schedule">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-12">
                        <br>
                        <h2 style="text-align: center;">SCHEDULE</h2>
                        <p style="text-align: center;">August 7th, 2023, 1:00 PM – 5:00 PM (Pacific Time), Long Beach, CA, USA.</p>
                            <div class="list-group list-group-flush">
                                <a class="list-group-item list-group-item-action">
                                  <div class="d-flex w-100 justify-content-between">
                                    <h5 class="mb-1"><span><i class="fas fa-book-open" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Opening</h5>
                                    <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;1:00 - 1:10 PM</h5>
                                  </div>
                                  <p class="mb-1">Introduction by organizers.</p>
                                </a>

                                
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fa-solid fa-paperclip" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Keynote Talk 1: Next-Generation Intelligent Assistants for AR/VR Devices</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;1:10 - 1:50 PM</h5>
                                    </div>
                                    <p class="mb-1"> Xin (Luna) Dong <i style="color:grey">Principal Scientist, Meta</i></p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;1:50 - 2:05 PM</h5>
                                    </div>
                                    <p class="mb-1"> Youxiang Zhu, Nana Lin, Xiaohui Liang, John Batsis, Robert Roth and Brian MacWhinney</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:05 - 2:20 PM</h5>
                                    </div>
                                    <p class="mb-1">Daniel Shin, Gao Pei, Priyadarshini Kumari and Tarek Besold</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Explainable Local and Global Models for Fine-Grained Multimodal Product Recognition</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:20 - 2:35 PM</h5>
                                    </div>
                                    <p class="mb-1">Tobias Pettersson, Maria Riveiro and Tuwe Löfström</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Contrastive Multimodal Text Generation for E-Commerce Brand Advertising</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:35 - 2:50 PM</h5>
                                    </div>
                                    <p class="mb-1">Nikhil Madaan, Krishna Kesari, Manisha Verma, Shaunak Mishra and Tor Steiner</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;What Makes Good Open-Vocabulary Detector: A Disassembling Perspective</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:50 - 3:05PM</h5>
                                    </div>
                                    <p class="mb-1">Jincheng Li, Chunyu Xie, Xiaoyu Wu, Bin Wang and Dawei Leng</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fa-solid fa-mug-hot" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Coffee Break</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;3:05 - 3:30 PM</h5>
                                    </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fa-solid fa-paperclip" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Keynote Talk 2: Graph-based Fusion for Multimodal Learning</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;3:30 - 4:10 PM</h5>
                                    </div>
                                    <p class="mb-1"> Aidong Zhang <i style="color:grey">Thomas M. Linville Endowed Professor of Computer Science, University of Virginia</i></p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Detecting text-rich objects: OCR or object detection? A case study with stopwatch detection</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;4:10 - 4:25 PM</h5>
                                    </div>
                                    <p class="mb-1">Yarong Feng, Zongyi Liu, Yuan Ling, Shunyan Luo, Shujing Dong, Shuyi Wang and Bruce Ferry</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;4:25 - 4:40 PM</h5>
                                    </div>
                                    <p class="mb-1">Ziniu Hu</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;4:40 - 4:55 PM</h5>
                                    </div>
                                    <p class="mb-1">Zheng Chen, Ziyan Jiang, Fan Yang, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu and Aram Galstyan
                                    </p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-book" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Closing</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;4:55-5:00 PM</h5>
                                    </div>
                                    <p class="mb-1">Concluding remarks by organizers.</p>
                                </a>
                            </div>             
                </div>
            </div>
        </section>


        <!-- Submission Guidelines section-->
        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="submission">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2 style="text-align: center;">Call for Contributions</h2>
                        <p style="text-align: justify;">
                            This workshop will provide a platform to discuss the latest advances and trends in theory, methodologies, and applications in the field of multimodal learning. The workshop theme for this year will be on the use of foundation models. These foundation models, such as BERT, T5, LLaMA and GPT-4 which were trained on massive data collections, have significantly revolutionized the field of natural language processing (NLP). The use of such foundation models for solving several NLP tasks represent a fundamental paradigm shift in the way several problems are being solved especially due to their ability to integrate knowledge from other domains such as computer vision (DALL-E, CLIP), retrieval, knowledge graphs and more. Moreover, foundation models have brought some fundamental changes to the multimodal problem setting, especially when integrating text or images with graphs, time-series, and other forms of structured data. As such, the workshop aims to focus on utilizing these foundation models and integrating multiple modalities. Though the workshop might also include discussions and papers about general multimodal learning problems, more emphasis will be given to the works that utilize recently developed foundation models. Our goal will be to explore and showcase the innovative ways in which multimodal learning and data fusion can be employed, with a particular emphasis on how to leverage the capabilities of foundation models for these purposes. The workshop topics include, but are not limited to:
                            <ul>
                                <li>Multimodal data generation</li>
                                <li>Multimodal data preprocessing and feature engineering</li>
                                <li>Multimodal data fusion</li>
                                <li>Multimodal self-supervised and/or unsupervised learning</li>
                                <li>Multimodal learning with noisy data</li>
                                <li>Multimodal transfer learning</li>
                                <li>Multimodal zero shot learning with foundation models </li>
                                <li>Biases in multimodal learning</li>
                                <li>Explainable multimodal learning</li>
                                <li>Multimodal generative AI</li>
                                <li>Trust-worthy multimodal learning </li>
                                <li>Large-scale multimodal learning  </li>
                                <li>Responsible multimodal learning </li>
                                <li>Applications of Multimodal learning (e.g., finance, healthcare, social media, climate, etc.)</li>
                            </ul>
                        </p>
                        <p style="text-align: justify;">
                            The workshop seeks to bring together researchers in the machine learning and data mining communities and provide a unique opportunity for interdisciplinary researchers to explore and data interactions with foundation models between various modalities, such as text, images, graphs, tabular data, time-series, and more. This workshop will feature invited talks, accepted paper presentations, and a panel discussion to encourage knowledge sharing and foster cross-team collaboration within research and industry communities in the fields of Natural Language Processing (NLP), Information Retrieval, Data Mining, Machine Learning, and others.
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Submission Dates section-->
        <section  style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="dates">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <br>
                        <h2 style="text-align: center;">Important Dates</h2>

                        <ul>
                            <li>Link to the submission website: <a href="https://easychair.org/my/conference?conf=multimodalkdd2023">https://easychair.org/my/conference?conf=multimodalkdd2023</a></li>
                            <li>All deadlines are given in Anywhere on Earth time.</li>
                        </ul>

                            <div class="list-group list-group-flush">
                                <!-- <a class="list-group-item list-group-item-action">
                                  <div class="d-flex w-100 justify-content-between">
                                    <h5 class="mb-1"><span><i class="fas fa-book-open" style="color:#0275d8;"></i></span><s>&nbsp;&nbsp;Paper Submission Deadline</s></h5>
                                    <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span><s>&nbsp;&nbsp;June 9th, 2023</s></h5>
                                  </div>
                                </a> -->
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-book-open" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Paper Submission Deadline</h5>
                                      <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;June 16th, 2023</h5>
                                    </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                  <div class="d-flex w-100 justify-content-between">
                                    <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Paper Acceptance Notification</h5>
                                    <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;July 10th, 2023</h5>
                                  </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-book" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Camera-Ready Submission</h5>
                                      <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;July 24th, 2023</h5>
                                    </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-circle-check" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Workshop Date</h5>
                                      <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;August 7th, 2023</h5>
                                    </div>
                                </a>
                            </div>             
                </div>
            </div>
        </section>        

        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="guidelines">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2 style="text-align: center;">Submission Guidelines</h2>
                        <p style="text-align: justify;">
                            <ul>
                                <li>Paper submissions are limited to 9 pages, excluding references, must be in PDF and use ACM Conference Proceeding templates (two column format). </li>
                                <li>Additional supplemental material focused on reproducibility can be provided. Proofs, pseudo-code, and code may also be included in the supplement, which has no explicit page limit. The supplement format could be either single column or double column. The paper should be self-contained, since reviewers are not required to read the supplement. </li>
                                <li>The Word template guideline can be found here: <a href="https://www.acm.org/publications/proceedings-template">[link]</a></li>
                                <li>The Latex/overleaf template guideline can be found here: <a href="https://www.overleaf.com/latex/templates/association-for-computing-machinery-acm-sig-proceedings-template/bmvfhcdnxfty">[link]</a></li></li>
                                <li>The submissions will be judged for quality and relevance through single-blind reviewing.</li>
                                
                                <li>A paper should be submitted in PDF format through EasyChair at the following link: <a href="https://easychair.org/my/conference?conf=multimodalkdd2023">[link]</a></li></li>
                            </ul> 
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Organizers section-->

        <!-- Footer-->
        <footer class="py-5 bg-dark">
            <div class="container px-4"><p class="m-0 text-center text-white">Copyright &copy; 2023</p></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <script src="https://kit.fontawesome.com/f3573b1642.js" crossorigin="anonymous"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
